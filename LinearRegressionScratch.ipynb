{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegressionScratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM5VtLkwGVSDOuMEbzd9bmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uankit/Linear_Model_Scratch/blob/main/LinearRegressionScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6hxkO8_12XH"
      },
      "source": [
        "# **Linear Regression From Scratch**\n",
        "    1. Change the number of observations to 100,000 and see what happens.\n",
        "    2. Change the number of observations to 1,000,000 and see what happens.\n",
        "    3. Play around with the learning rate. Values like 0.0001, 0.001, 0.1, 1 are all interesting to observe. \n",
        "    4. Change the loss function. L2-norm loss (without dividing by 2) is a good way to start. \n",
        "    5. Ð¢ry with the L1-norm loss, given by the sum of the ABSOLUTE value of yj - tj. The L1-norm loss is given by:\n",
        "## $$ \\Sigma_i = |y_i-t_i| $$\n",
        "    6. Create a function f(x0,x1,x2) = 8*x0 + 2*x1 - 4*x2 + 5. Does the algorithm work in the same way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi8NoDL2cBP"
      },
      "source": [
        "### **Import the relevant libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_PcUGgCUYHS"
      },
      "source": [
        "# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib for the sole purpose of visualizing the results.  \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya9LTyMk2uSv"
      },
      "source": [
        "### **Generate random input data to train on**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP1oQIdvUY6c",
        "outputId": "94e80c21-0151-484f-b3ca-1ea9a5462dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# First, we should declare a variable containing the size of the training set we want to generate.\n",
        "\n",
        "obs = 5000\n",
        "    \n",
        "# We will work with three variables as inputs.\n",
        "# We generate them randomly, drawing from an uniform distribution.\n",
        "# The size of x0 , x1 and x2 is observations by 1. In this case: 5000 x 1.\n",
        "\n",
        "x0 = np.random.uniform(-10 , 10 , (obs,1))\n",
        "x1 = np.random.uniform(-10 , 10 , (obs,1))\n",
        "x2 = np.random.uniform(-10 , 10 , (obs,1))\n",
        "\n",
        "# Combine the two dimensions of the input into one input matrix. \n",
        "# This is the X matrix from the linear model y = x*w + b.\n",
        "# column_stack is a Numpy method, which combines two vectors into a matrix.\n",
        "\n",
        "inputs = np.column_stack((x0,x1,x2))\n",
        "print(inputs.shape)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbZd2fnR3gFR"
      },
      "source": [
        "### **Generate the targets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw1NQwEZUZH3"
      },
      "source": [
        "# We want to \"make up\" a function, use the ML methodology, and see if the algorithm has learned it.\n",
        "# We add a small random noise to the function i.e. f(x0,x1,x2) = 8*x0 + 2*x1 - 4*x2 + 5  + <small noise>\n",
        "\n",
        "noise = np.random.uniform(-2,2,(obs,1))\n",
        "\n",
        "# Produce the targets according to the f(x0,x1,x2) = 8*x0 + 2*x1 - 4*x2 + 5  + noise definition.\n",
        "# In this way, we are basically saying: the weights should be 8 , 2 and -4, while the bias is 5.\n",
        "\n",
        "target =  8*x0 + 2*x1 - 4*x2 + 5 + noise"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkFvheM54VvJ"
      },
      "source": [
        "### **Initialize variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP0MFJ3bUZMs",
        "outputId": "c60d4c6f-8079-450e-c049-4eb3ad620b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Weights are of size k x m, where k is the number of input variables and m is the number of output variables\n",
        "\n",
        "weights = np.random.uniform(-1,1,size=(3,1))\n",
        "\n",
        "# Biases are of size 1 since there is only 1 output. The bias is a scalar.\n",
        "\n",
        "bias = np.random.uniform(-1,1,size=1)\n",
        "\n",
        "print(weights)\n",
        "print(bias)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.98116898]\n",
            " [ 0.75552891]\n",
            " [-0.50497357]]\n",
            "[-0.98168236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGZCWpE5qS5"
      },
      "source": [
        "### **Set a learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc7zz04rUZUt"
      },
      "source": [
        "# Set some small learning rate . \n",
        "# 0.02 is going to work quite well for our example.\n",
        "\n",
        "learning_rate = 0.02"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVqxssqo6Bx9"
      },
      "source": [
        "### **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kJUdDNf59Ww",
        "outputId": "b048b0e2-658c-47f4-eb69-5b91e88c1f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.\n",
        "# a lower learning rate would need more iterations, while a higher learning rate would need less iterations\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  # This is the linear model: y = xw + b equation\n",
        "\n",
        "  output = np.dot(inputs , weights)+bias\n",
        "\n",
        "  # The deltas are the differences between the outputs and the targets\n",
        "\n",
        "  deltas = output - target\n",
        "\n",
        "  # We are considering the L2-norm loss, but divided by 2.\n",
        "  # Moreover, we further divide it by the number of observations.\n",
        "  # This is simple rescaling by a constant. This doesn't change the optimization logic,\n",
        "  # as any function holding the basic property of being lower for better results, and higher for worse results\n",
        "  # can be a loss function.\n",
        "\n",
        "  loss = np.sum(deltas**2)/2/obs\n",
        "\n",
        "  # We print the loss function value at each step so we can observe whether it is decreasing as desired.\n",
        "\n",
        "  print(loss)\n",
        "\n",
        "  # Another small trick is to scale the deltas the same way as the loss function\n",
        "  # In this way our learning rate is independent of the number of samples (observations).\n",
        "  # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate\n",
        "  # that can remain the same if we change the number of training samples (observations).\n",
        "\n",
        "  deltas_scaled = deltas/obs\n",
        "\n",
        "  # Finally, we must apply the gradient descent update rules\n",
        "  \n",
        "  weights = weights - learning_rate*np.dot(inputs.T , deltas_scaled)\n",
        "  bias = bias - learning_rate * np.sum(deltas_scaled)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1570.0435198914793\n",
            "197.1737793403677\n",
            "38.045350973824455\n",
            "19.049338563118418\n",
            "16.290633031222743\n",
            "15.43666551575106\n",
            "14.825085376834442\n",
            "14.261983825833624\n",
            "13.723998776079856\n",
            "13.207637633552821\n",
            "12.711753012976608\n",
            "12.235500389846338\n",
            "11.778098733726418\n",
            "11.33880144316067\n",
            "10.916891879088482\n",
            "10.511681815739355\n",
            "10.122510273485382\n",
            "9.748742435434668\n",
            "9.389768611306023\n",
            "9.045003242830203\n",
            "8.71388394857847\n",
            "8.395870606604815\n",
            "8.09044447339868\n",
            "7.797107337710276\n",
            "7.515380707868133\n",
            "7.244805031263325\n",
            "6.98493894472697\n",
            "6.735358554578472\n",
            "6.4956567451699625\n",
            "6.26544251479903\n",
            "6.044340337906567\n",
            "5.83198955251934\n",
            "5.628043771937973\n",
            "5.43217031971082\n",
            "5.244049686971984\n",
            "5.063375011258303\n",
            "4.889851575955155\n",
            "4.723196329554604\n",
            "4.563137423941597\n",
            "4.409413770955248\n",
            "4.261774616501691\n",
            "4.119979131523871\n",
            "3.983796019161078\n",
            "3.853003137457415\n",
            "3.7273871370037517\n",
            "3.6067431129220116\n",
            "3.4908742706243028\n",
            "3.379591604801462\n",
            "3.272713591117497\n",
            "3.1700658901070495\n",
            "3.0714810627927447\n",
            "2.9767982975586347\n",
            "2.885863147834281\n",
            "2.7985272801613763\n",
            "2.7146482322321535\n",
            "2.6340891805048354\n",
            "2.5567187170169694\n",
            "2.4824106350327115\n",
            "2.4110437231743522\n",
            "2.3425015677022447\n",
            "2.276672362620676\n",
            "2.21344872729986\n",
            "2.1527275313166028\n",
            "2.0944097262279295\n",
            "2.0384001840031973\n",
            "1.984607541851209\n",
            "1.932944053189201\n",
            "1.8833254445105634\n",
            "1.8356707779178651\n",
            "1.7899023190969106\n",
            "1.7459454105164987\n",
            "1.7037283496470088\n",
            "1.6631822719992209\n",
            "1.6242410387925177\n",
            "1.5868411290692703\n",
            "1.5509215360794448\n",
            "1.5164236677663248\n",
            "1.483291251191164\n",
            "1.451470240740718\n",
            "1.4209087299680085\n",
            "1.3915568669225267\n",
            "1.3633667728317003\n",
            "1.3362924640009948\n",
            "1.310289776805322\n",
            "1.285316295649302\n",
            "1.2613312837789143\n",
            "1.2382956168316999\n",
            "1.216171719017076\n",
            "1.1949235018226914\n",
            "1.174516305146827\n",
            "1.1549168407608255\n",
            "1.136093138009312\n",
            "1.1180144916596488\n",
            "1.1006514118155413\n",
            "1.0839755758131233\n",
            "1.0679597820209838\n",
            "1.0525779054688815\n",
            "1.037804855232672\n",
            "1.0236165335059697\n",
            "1.009989796291811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG9J64Sd7PjW"
      },
      "source": [
        "### **Print weights and biases and see if we have worked correctly.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAdK9jihUZcz",
        "outputId": "07dd4b8a-c519-46f5-efb6-c68f1e6c2dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# We print the weights and the biases, so we can see if they have converged to what we wanted.\n",
        "\n",
        "print(weights,bias)\n",
        "\n",
        "# Note that they may be convergING. So more iterations are needed."
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7.9956069 ]\n",
            " [ 1.99705419]\n",
            " [-4.00000549]] [4.19334287]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pERMcvH8P28"
      },
      "source": [
        "```\n",
        "Here the weights are as follows : 7.99 , 1.99 , -4.00 , 4.19 \n",
        "which is very close to our desired target equation weights : 8.00 , 2.00 , -4.00 , 5.00\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8OwiZWx7gCP"
      },
      "source": [
        "### **Plot last outputs vs targets**\n",
        "Since they are the last ones at the end of the training, they represent the final model accuracy. <br/>\n",
        "The closer this plot is to a 45 degree line, the closer target and output values are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp-pXt1WUZfp",
        "outputId": "9ab83f86-e578-40bd-cbc4-46b72b43d9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# We print the outputs and the targets in order to see if they have a linear relationship.\n",
        "\n",
        "plt.plot(output,target)\n",
        "plt.xlabel('outputs')\n",
        "plt.ylabel('targets')\n",
        "plt.show()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}